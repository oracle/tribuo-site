---
title: "Regression"
og-title: "Regression Tutorial"
learn_nav: true
parent: Tutorials
nav_order: 304
is_notebook: true
notebook_url: https://github.com/oracle/tribuo/blob/main/tutorials/regression-tribuo-v4.ipynb
comment: ## DO NOT EDIT THIS FILE. IT IS COPIED FROM THE TRIBUO DOC. EDIT IT THERE. ##
---
<main>
<div class="border-box-sizing" id="notebook" tabindex="-1">
<div class="container" id="notebook-container">
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Regression-Tutorial">Regression Tutorial<a class="anchor-link" href="#Regression-Tutorial">¶</a></h1><p>This guide will show how to use Tribuo’s regression models to predict wine quality based on the <a href="https://archive.ics.uci.edu/ml/datasets/Wine+Quality">UCI Wine Quality</a> data set. We’ll experiment with several different regression trainers: two for training linear models (SGD and Adagrad) and one for training a tree ensemble via Tribuo’s wrapper on XGBoost. We’ll run these experiments by simply swapping in different implementations of Tribuo’s <code>Trainer</code> interface. We’ll also show how to evaluate regression models and describe some common evaluation metrics.</p>
<h2 id="Setup">Setup<a class="anchor-link" href="#Setup">¶</a></h2><p>First you'll need to download the winequality dataset from UCI:</p>
<p><code>wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv</code></p>
<p>then we'll load in some jars and import a few packages.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [1]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-java"><pre><span></span><span class="o">%</span><span class="n">jars</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">tribuo</span><span class="o">-</span><span class="n">json</span><span class="o">-</span><span class="mf">4.0.0</span><span class="o">-</span><span class="n">SNAPSHOT</span><span class="o">-</span><span class="n">jar</span><span class="o">-</span><span class="n">with</span><span class="o">-</span><span class="n">dependencies</span><span class="p">.</span><span class="na">jar</span>
<span class="o">%</span><span class="n">jars</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">tribuo</span><span class="o">-</span><span class="n">regression</span><span class="o">-</span><span class="n">sgd</span><span class="o">-</span><span class="mf">4.0.0</span><span class="o">-</span><span class="n">SNAPSHOT</span><span class="o">-</span><span class="n">jar</span><span class="o">-</span><span class="n">with</span><span class="o">-</span><span class="n">dependencies</span><span class="p">.</span><span class="na">jar</span>
<span class="o">%</span><span class="n">jars</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">tribuo</span><span class="o">-</span><span class="n">regression</span><span class="o">-</span><span class="n">xgboost</span><span class="o">-</span><span class="mf">4.0.0</span><span class="o">-</span><span class="n">SNAPSHOT</span><span class="o">-</span><span class="n">jar</span><span class="o">-</span><span class="n">with</span><span class="o">-</span><span class="n">dependencies</span><span class="p">.</span><span class="na">jar</span>
<span class="o">%</span><span class="n">jars</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">tribuo</span><span class="o">-</span><span class="n">regression</span><span class="o">-</span><span class="n">tree</span><span class="o">-</span><span class="mf">4.0.0</span><span class="o">-</span><span class="n">SNAPSHOT</span><span class="o">-</span><span class="n">jar</span><span class="o">-</span><span class="n">with</span><span class="o">-</span><span class="n">dependencies</span><span class="p">.</span><span class="na">jar</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [2]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-java"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">java.nio.file.Path</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">java.nio.file.Paths</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [3]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-java"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">org.tribuo.*</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.tribuo.data.csv.CSVLoader</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.tribuo.datasource.ListDataSource</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.tribuo.evaluation.TrainTestSplitter</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.tribuo.math.optimisers.*</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.tribuo.regression.*</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.tribuo.regression.evaluation.*</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.tribuo.regression.sgd.RegressionObjective</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.tribuo.regression.sgd.linear.LinearSGDTrainer</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.tribuo.regression.sgd.objectives.SquaredLoss</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.tribuo.regression.rtree.CARTRegressionTrainer</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.tribuo.regression.xgboost.XGBoostRegressionTrainer</span><span class="p">;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">org.tribuo.util.Util</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Loading-the-data">Loading the data<a class="anchor-link" href="#Loading-the-data">¶</a></h2><p>In Tribuo, all the prediction types have an associated <code>OutputFactory</code> implementation, which can create the appropriate <code>Output</code> subclasses from an input. Here we're going to use <code>RegressionFactory</code> as we're performing regression. In Tribuo both single and multidimensional regression use the <code>Regressor</code> and <code>RegressionFactory</code> classes. We then pass the <code>regressionFactory</code> into the simple <code>CSVLoader</code> which reads all the columns into a <code>DataSource</code>. The winequality dataset uses <code>;</code> to separate the columns rather than the standard <code>,</code>.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [4]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-java"><pre><span></span><span class="kd">var</span><span class="w"> </span><span class="n">regressionFactory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">RegressionFactory</span><span class="p">();</span>
<span class="kd">var</span><span class="w"> </span><span class="n">csvLoader</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">CSVLoader</span><span class="o">&lt;&gt;</span><span class="p">(</span><span class="sc">';'</span><span class="p">,</span><span class="n">regressionFactory</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We don't have a pre-defined train test split, so we take 70% as the training data, and 30% as the test data. The data is randomised using the RNG seeded by the second value. Then we feed the split data sources into the training and testing datasets. These <code>MutableDataset</code>s manage all the metadata (e.g. feature &amp; output domains), and the mapping from feature names to feature id numbers.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [5]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-java"><pre><span></span><span class="kd">var</span><span class="w"> </span><span class="n">wineSource</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">csvLoader</span><span class="p">.</span><span class="na">loadDataSource</span><span class="p">(</span><span class="n">Paths</span><span class="p">.</span><span class="na">get</span><span class="p">(</span><span class="s">"winequality-red.csv"</span><span class="p">),</span><span class="s">"quality"</span><span class="p">);</span>
<span class="kd">var</span><span class="w"> </span><span class="n">splitter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">TrainTestSplitter</span><span class="o">&lt;&gt;</span><span class="p">(</span><span class="n">wineSource</span><span class="p">,</span><span class="w"> </span><span class="mf">0.7f</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="n">L</span><span class="p">);</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Regressor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">trainData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MutableDataset</span><span class="o">&lt;&gt;</span><span class="p">(</span><span class="n">splitter</span><span class="p">.</span><span class="na">getTrain</span><span class="p">());</span>
<span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Regressor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">evalData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MutableDataset</span><span class="o">&lt;&gt;</span><span class="p">(</span><span class="n">splitter</span><span class="p">.</span><span class="na">getTest</span><span class="p">());</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Training-the-models">Training the models<a class="anchor-link" href="#Training-the-models">¶</a></h2><p>We're going to define a quick training function which accepts a trainer and a training dataset. It times the training and also prints the performance metrics. Evaluating on the training data is useful for debugging: if the model performs poorly in the training data, then we know something is wrong.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [6]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-java"><pre><span></span><span class="kd">public</span><span class="w"> </span><span class="n">Model</span><span class="o">&lt;</span><span class="n">Regressor</span><span class="o">&gt;</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">String</span><span class="w"> </span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">Trainer</span><span class="o">&lt;</span><span class="n">Regressor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">trainer</span><span class="p">,</span><span class="w"> </span><span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Regressor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">trainData</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Train the model</span>
<span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="n">startTime</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">System</span><span class="p">.</span><span class="na">currentTimeMillis</span><span class="p">();</span>
<span class="w">    </span><span class="n">Model</span><span class="o">&lt;</span><span class="n">Regressor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">trainer</span><span class="p">.</span><span class="na">train</span><span class="p">(</span><span class="n">trainData</span><span class="p">);</span>
<span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="n">endTime</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">System</span><span class="p">.</span><span class="na">currentTimeMillis</span><span class="p">();</span>
<span class="w">    </span><span class="n">System</span><span class="p">.</span><span class="na">out</span><span class="p">.</span><span class="na">println</span><span class="p">(</span><span class="s">"Training "</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s">" took "</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Util</span><span class="p">.</span><span class="na">formatDuration</span><span class="p">(</span><span class="n">startTime</span><span class="p">,</span><span class="n">endTime</span><span class="p">));</span>
<span class="w">    </span><span class="c1">// Evaluate the model on the training data (this is a useful debugging tool)</span>
<span class="w">    </span><span class="n">RegressionEvaluator</span><span class="w"> </span><span class="n">eval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">RegressionEvaluator</span><span class="p">();</span>
<span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="n">evaluation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">eval</span><span class="p">.</span><span class="na">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">trainData</span><span class="p">);</span>
<span class="w">    </span><span class="c1">// We create a dimension here to aid pulling out the appropriate statistics.</span>
<span class="w">    </span><span class="c1">// You can also produce the String directly by calling "evaluation.toString()"</span>
<span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="n">dimension</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">Regressor</span><span class="p">(</span><span class="s">"DIM-0"</span><span class="p">,</span><span class="n">Double</span><span class="p">.</span><span class="na">NaN</span><span class="p">);</span>
<span class="w">    </span><span class="n">System</span><span class="p">.</span><span class="na">out</span><span class="p">.</span><span class="na">printf</span><span class="p">(</span><span class="s">"Evaluation (train):%n  RMSE %f%n  MAE %f%n  R^2 %f%n"</span><span class="p">,</span>
<span class="w">            </span><span class="n">evaluation</span><span class="p">.</span><span class="na">rmse</span><span class="p">(</span><span class="n">dimension</span><span class="p">),</span><span class="w"> </span><span class="n">evaluation</span><span class="p">.</span><span class="na">mae</span><span class="p">(</span><span class="n">dimension</span><span class="p">),</span><span class="w"> </span><span class="n">evaluation</span><span class="p">.</span><span class="na">r2</span><span class="p">(</span><span class="n">dimension</span><span class="p">));</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">model</span><span class="p">;</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we're going to define an equivalent testing function which accepts a model and a test dataset, printing the performance to std out.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [7]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-java"><pre><span></span><span class="kd">public</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span><span class="n">Model</span><span class="o">&lt;</span><span class="n">Regressor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">model</span><span class="p">,</span><span class="w"> </span><span class="n">Dataset</span><span class="o">&lt;</span><span class="n">Regressor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">testData</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">// Evaluate the model on the test data</span>
<span class="w">    </span><span class="n">RegressionEvaluator</span><span class="w"> </span><span class="n">eval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">RegressionEvaluator</span><span class="p">();</span>
<span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="n">evaluation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">eval</span><span class="p">.</span><span class="na">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">testData</span><span class="p">);</span>
<span class="w">    </span><span class="c1">// We create a dimension here to aid pulling out the appropriate statistics.</span>
<span class="w">    </span><span class="c1">// You can also produce the String directly by calling "evaluation.toString()"</span>
<span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="n">dimension</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">Regressor</span><span class="p">(</span><span class="s">"DIM-0"</span><span class="p">,</span><span class="n">Double</span><span class="p">.</span><span class="na">NaN</span><span class="p">);</span>
<span class="w">    </span><span class="n">System</span><span class="p">.</span><span class="na">out</span><span class="p">.</span><span class="na">printf</span><span class="p">(</span><span class="s">"Evaluation (test):%n  RMSE %f%n  MAE %f%n  R^2 %f%n"</span><span class="p">,</span>
<span class="w">            </span><span class="n">evaluation</span><span class="p">.</span><span class="na">rmse</span><span class="p">(</span><span class="n">dimension</span><span class="p">),</span><span class="w"> </span><span class="n">evaluation</span><span class="p">.</span><span class="na">mae</span><span class="p">(</span><span class="n">dimension</span><span class="p">),</span><span class="w"> </span><span class="n">evaluation</span><span class="p">.</span><span class="na">r2</span><span class="p">(</span><span class="n">dimension</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we'll define the four trainers we're going to compare.</p>
<ul>
<li>A linear regression trained using linear decay SGD.</li>
<li>A linear regression trained using SGD and AdaGrad.</li>
<li>A regression tree using the CART algorithm with a maximum depth of 6.</li>
<li>An XGBoost trainer using 50 rounds of boosting.</li>
</ul>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [8]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-java"><pre><span></span><span class="kd">var</span><span class="w"> </span><span class="n">lrsgd</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">LinearSGDTrainer</span><span class="p">(</span>
<span class="w">    </span><span class="k">new</span><span class="w"> </span><span class="n">SquaredLoss</span><span class="p">(),</span><span class="w"> </span><span class="c1">// loss function</span>
<span class="w">    </span><span class="n">SGD</span><span class="p">.</span><span class="na">getLinearDecaySGD</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span><span class="w"> </span><span class="c1">// gradient descent algorithm</span>
<span class="w">    </span><span class="mi">10</span><span class="p">,</span><span class="w">                </span><span class="c1">// number of training epochs</span>
<span class="w">    </span><span class="n">trainData</span><span class="p">.</span><span class="na">size</span><span class="p">()</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span><span class="c1">// logging interval</span>
<span class="w">    </span><span class="mi">1</span><span class="p">,</span><span class="w">                 </span><span class="c1">// minibatch size</span>
<span class="w">    </span><span class="mi">1L</span><span class="w">                 </span><span class="c1">// RNG seed</span>
<span class="p">);</span>
<span class="kd">var</span><span class="w"> </span><span class="n">lrada</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">LinearSGDTrainer</span><span class="p">(</span>
<span class="w">    </span><span class="k">new</span><span class="w"> </span><span class="n">SquaredLoss</span><span class="p">(),</span>
<span class="w">    </span><span class="k">new</span><span class="w"> </span><span class="n">AdaGrad</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span>
<span class="w">    </span><span class="mi">10</span><span class="p">,</span>
<span class="w">    </span><span class="n">trainData</span><span class="p">.</span><span class="na">size</span><span class="p">()</span><span class="o">/</span><span class="mi">4</span><span class="p">,</span>
<span class="w">    </span><span class="mi">1</span><span class="p">,</span>
<span class="w">    </span><span class="mi">1L</span><span class="w"> </span>
<span class="p">);</span>
<span class="kd">var</span><span class="w"> </span><span class="n">cart</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">CARTRegressionTrainer</span><span class="p">(</span><span class="mi">6</span><span class="p">);</span>
<span class="kd">var</span><span class="w"> </span><span class="n">xgb</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">XGBoostRegressionTrainer</span><span class="p">(</span><span class="mi">50</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we'll train the linear regression with SGD:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [9]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-java"><pre><span></span><span class="kd">var</span><span class="w"> </span><span class="n">lrsgdModel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">(</span><span class="s">"Linear Regression (SGD)"</span><span class="p">,</span><span class="n">lrsgd</span><span class="p">,</span><span class="n">trainData</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Training Linear Regression (SGD) took (00:00:00:132)
Evaluation (train):
  RMSE 0.979522
  MAE 0.741870
  R^2 -0.471611
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Evaluating-the-models">Evaluating the models<a class="anchor-link" href="#Evaluating-the-models">¶</a></h2><p>Using our evaluation function this is pretty straightforward.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [10]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-java"><pre><span></span><span class="n">evaluate</span><span class="p">(</span><span class="n">lrsgdModel</span><span class="p">,</span><span class="n">evalData</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Evaluation (test):
  RMSE 0.967450
  MAE 0.720619
  R^2 -0.439255
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Those numbers seem poor, but what do these evaluation metrics mean?</p>
<h3 id="RMSE">RMSE<a class="anchor-link" href="#RMSE">¶</a></h3><p>The root-mean-square error (RMSE) summarizes the magnitude of errors between our regression model's predictions and the values we observe in our data. Basically, RMSE is the standard deviation of model prediction errors on a given dataset.</p>
<p>$$RMSE = \sqrt{ \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 }$$</p>
<p>Lower is better: a perfect model for the wine data would have RMSE=0. The RMSE is sensitive to how large an error was, and is thus sensitive to outliers. This also means that RMSE can be used to compare different models on the same dataset but not across different datasets, as a "good" RMSE value on one dataset might be larger than a "good" RMSE value on a different dataset. See <a href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">Wikipedia</a> for more info on RMSE.</p>
<h3 id="MAE">MAE<a class="anchor-link" href="#MAE">¶</a></h3><p>The mean absolute error (MAE) is another summary of model error. Unlike RMSE, each error in MAE contributes proportional to its absolute value.</p>
<p>$$MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$</p>
<h3 id="R%5E2">R^2<a class="anchor-link" href="#R%5E2">¶</a></h3><p>The R-squared metric (also called the "coefficient of determination") summarizes how much of the variation in observed outcomes can be explained by our model.</p>
<p>Let $\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$, i.e. the mean deviation of observed data points from the observed mean. R^2 is given by:</p>
<p>$$R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$$</p>
<p>A value of R^2=1 means that the model accounts for all of the variation in a set of observations -- in other words, it fits a dataset perfectly. Note that R^2 can turn negative when the sum-of-squared model errors (numerator) is greater than the sum-of-squared differences between observed data points and the observed mean (denominator). In other words, when R^2 is negative, the model fits the data <em>worse</em> than simply using the observed mean to predict values.</p>
<p>See <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination">Wikipedia</a> and the <a href="https://blog.minitab.com/blog/adventures-in-statistics-2/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit">Minitab blog</a> for more detailed discussion of R^2.</p>
<h2 id="Improving-over-standard-SGD-with-AdaGrad">Improving over standard SGD with AdaGrad<a class="anchor-link" href="#Improving-over-standard-SGD-with-AdaGrad">¶</a></h2><p>It's not surprising the SGD results are bad: in linear decay SGD, the step size used for parameter updates changes over time (training iterations) but is uniform across all model parameters. This means that we use the same step size for a noisy/irrelevant feature as we would for an informative feature. There many more sophisticated approaches to gradient descent.</p>
<p>One of these is <a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">AdaGrad</a>, modifies the "global" learning rate for each parameter $p$ using the sum-of-squares of past gradients w.r.t. $p$, up to time $t$.</p>
<blockquote>
<p>... the secret sauce of AdaGrad is not on necessarily accelerating gradient descent with a better step size selection, but making gradient descent more stable to not-so-good (\eta) choices.
Anastasios Kyrillidis, <a href="http://akyrillidis.github.io/notes/AdaGrad">Note on AdaGrad</a></p>
</blockquote>
<p>Let's try training for the same number of epochs using <code>AdaGrad</code> instead of <code>LinearDecaySGD</code>:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [11]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-java"><pre><span></span><span class="kd">var</span><span class="w"> </span><span class="n">lradaModel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">(</span><span class="s">"Linear Regression (AdaGrad)"</span><span class="p">,</span><span class="n">lrada</span><span class="p">,</span><span class="n">trainData</span><span class="p">);</span>
<span class="n">evaluate</span><span class="p">(</span><span class="n">lradaModel</span><span class="p">,</span><span class="n">evalData</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Training Linear Regression (AdaGrad) took (00:00:00:078)
Evaluation (train):
  RMSE 0.735311
  MAE 0.575096
  R^2 0.170709
Evaluation (test):
  RMSE 0.737994
  MAE 0.585709
  R^2 0.162497
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using a more robust optimizer got us a better fit in the same number of epochs. However, both the train and test R^2 scores are still substantially less than 1 and, as before, the train and test RMSE scores are very similar.</p>
<p>See <a href="http://akyrillidis.github.io/notes/AdaGrad">here</a> and <a href="http://ruder.io/optimizing-gradient-descent/index.html#adagrad">here</a> for more on AdaGrad. Also, there are many other implementations of various well-known optimizers in Tribuo, including <a href="https://tribuo.org/javadoc/v4.0.0/org/tribuo/math/optimisers/Adam.html">Adam</a> and <a href="https://tribuo.org/javadoc/v4.0.0/org/tribuo/math/optimisers/RMSProp.html">RMSProp</a>. See the <a href="https://tribuo.org/javadoc/v4.0.0/org/tribuo/math/optimisers/package-summary.html">math.optimisers</a> package.</p>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>At this point, we showed that we can improve our model by using a more robust optimizer; however, we're still using a linear model. If there are informative, non-linear relationships among wine quality features, then our current model won't be able to take advantage of them. We'll finish this tutorial by showing how to use a couple of popular non-linear models, CART and <a href="https://xgboost.ai">XGBoost</a>.</p>
<h2 id="Trees-and-ensembles">Trees and ensembles<a class="anchor-link" href="#Trees-and-ensembles">¶</a></h2><p>Next we'll train the CART tree:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [12]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-java"><pre><span></span><span class="kd">var</span><span class="w"> </span><span class="n">cartModel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">(</span><span class="s">"CART"</span><span class="p">,</span><span class="n">cart</span><span class="p">,</span><span class="n">trainData</span><span class="p">);</span>
<span class="n">evaluate</span><span class="p">(</span><span class="n">cartModel</span><span class="p">,</span><span class="n">evalData</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Training CART took (00:00:00:076)
Evaluation (train):
  RMSE 0.545205
  MAE 0.406670
  R^2 0.544085
Evaluation (test):
  RMSE 0.657900
  MAE 0.494812
  R^2 0.334420
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally we'll train the XGBoost ensemble:</p>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In [13]:</div>
<div class="inner_cell">
<div class="input_area">
<div class="highlight hl-java"><pre><span></span><span class="kd">var</span><span class="w"> </span><span class="n">xgbModel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">(</span><span class="s">"XGBoost"</span><span class="p">,</span><span class="n">xgb</span><span class="p">,</span><span class="n">trainData</span><span class="p">);</span>
<span class="n">evaluate</span><span class="p">(</span><span class="n">xgbModel</span><span class="p">,</span><span class="n">evalData</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="output_wrapper">
<div class="output">
<div class="output_area">
<div class="prompt"></div>
<div class="output_subarea output_stream output_stdout output_text">
<pre>Training XGBoost took (00:00:00:341)
Evaluation (train):
  RMSE 0.143871
  MAE 0.097167
  R^2 0.968252
Evaluation (test):
  RMSE 0.599478
  MAE 0.426673
  R^2 0.447378
</pre>
</div>
</div>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using gradient boosting via XGBoost improved results by a lot. Not only are the train &amp; test fits better, but the train and test RMSE have started to diverge, indicating that the XGBoost model isn't underfitting like the previous two linear models were. XGBoost won't always be the best model for your data, but it's often a great baseline model to try when facing a new problem or dataset.</p>
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion">¶</a></h2><p>In this tutorial, we showed how to experiment with several different regression trainers (linear decay SGD, AdaGrad, CART, XGBoost). It was easy to experiment with different trainers and models by simply swapping in different implementations of the Tribuo <code>Trainer</code> interface. We also showed how to evaluate regression models and described some common evaluation metrics.</p>
</div>
</div>
</div>
</div>
</div>
</main>
