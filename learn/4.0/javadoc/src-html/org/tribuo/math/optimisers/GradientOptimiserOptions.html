<!DOCTYPE HTML>
<html lang="en">
<head>
<!-- Generated by javadoc (24) -->
<title>Source code</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="source: package: org.tribuo.math.optimisers, class: GradientOptimiserOptions">
<meta name="generator" content="javadoc/SourceToHTMLConverter">
<link rel="stylesheet" type="text/css" href="../../../../../resource-files/stylesheet.css" title="Style">
</head>
<body class="source-page">
<main role="main">
<div class="source-container">
<pre><span class="source-line-no">001</span><span id="line-1">/*</span>
<span class="source-line-no">002</span><span id="line-2"> * Copyright (c) 2015-2020, Oracle and/or its affiliates. All rights reserved.</span>
<span class="source-line-no">003</span><span id="line-3"> *</span>
<span class="source-line-no">004</span><span id="line-4"> * Licensed under the Apache License, Version 2.0 (the "License");</span>
<span class="source-line-no">005</span><span id="line-5"> * you may not use this file except in compliance with the License.</span>
<span class="source-line-no">006</span><span id="line-6"> * You may obtain a copy of the License at</span>
<span class="source-line-no">007</span><span id="line-7"> *</span>
<span class="source-line-no">008</span><span id="line-8"> *     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="source-line-no">009</span><span id="line-9"> *</span>
<span class="source-line-no">010</span><span id="line-10"> * Unless required by applicable law or agreed to in writing, software</span>
<span class="source-line-no">011</span><span id="line-11"> * distributed under the License is distributed on an "AS IS" BASIS,</span>
<span class="source-line-no">012</span><span id="line-12"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express implied.</span>
<span class="source-line-no">013</span><span id="line-13"> * See the License for the specific language governing permissions and</span>
<span class="source-line-no">014</span><span id="line-14"> * limitations under the License.</span>
<span class="source-line-no">015</span><span id="line-15"> */</span>
<span class="source-line-no">016</span><span id="line-16"></span>
<span class="source-line-no">017</span><span id="line-17">package org.tribuo.math.optimisers;</span>
<span class="source-line-no">018</span><span id="line-18"></span>
<span class="source-line-no">019</span><span id="line-19">import com.oracle.labs.mlrg.olcut.config.Option;</span>
<span class="source-line-no">020</span><span id="line-20">import com.oracle.labs.mlrg.olcut.config.Options;</span>
<span class="source-line-no">021</span><span id="line-21">import org.tribuo.math.StochasticGradientOptimiser;</span>
<span class="source-line-no">022</span><span id="line-22"></span>
<span class="source-line-no">023</span><span id="line-23">import java.util.logging.Logger;</span>
<span class="source-line-no">024</span><span id="line-24"></span>
<span class="source-line-no">025</span><span id="line-25">/**</span>
<span class="source-line-no">026</span><span id="line-26"> * CLI options for configuring a gradient optimiser.</span>
<span class="source-line-no">027</span><span id="line-27"> */</span>
<span class="source-line-no">028</span><span id="line-28">public class GradientOptimiserOptions implements Options {</span>
<span class="source-line-no">029</span><span id="line-29">    private static final Logger logger = Logger.getLogger(GradientOptimiserOptions.class.getName());</span>
<span class="source-line-no">030</span><span id="line-30"></span>
<span class="source-line-no">031</span><span id="line-31">    /**</span>
<span class="source-line-no">032</span><span id="line-32">     * Type of the gradient optimisers available in CLIs.</span>
<span class="source-line-no">033</span><span id="line-33">     */</span>
<span class="source-line-no">034</span><span id="line-34">    public enum StochasticGradientOptimiserType {</span>
<span class="source-line-no">035</span><span id="line-35">        ADADELTA,</span>
<span class="source-line-no">036</span><span id="line-36">        ADAGRAD,</span>
<span class="source-line-no">037</span><span id="line-37">        ADAGRADRDA,</span>
<span class="source-line-no">038</span><span id="line-38">        ADAM,</span>
<span class="source-line-no">039</span><span id="line-39">        PEGASOS,</span>
<span class="source-line-no">040</span><span id="line-40">        RMSPROP,</span>
<span class="source-line-no">041</span><span id="line-41">        CONSTANTSGD,</span>
<span class="source-line-no">042</span><span id="line-42">        LINEARSGD,</span>
<span class="source-line-no">043</span><span id="line-43">        SQRTSGD</span>
<span class="source-line-no">044</span><span id="line-44">    }</span>
<span class="source-line-no">045</span><span id="line-45"></span>
<span class="source-line-no">046</span><span id="line-46">    @Option(longName = "sgo-type", usage = "Selects the gradient optimiser. Defaults to ADAGRAD.")</span>
<span class="source-line-no">047</span><span id="line-47">    private StochasticGradientOptimiserType optimiserType = StochasticGradientOptimiserType.ADAGRAD;</span>
<span class="source-line-no">048</span><span id="line-48"></span>
<span class="source-line-no">049</span><span id="line-49">    @Option(longName = "sgo-learning-rate", usage = "Learning rate for AdaGrad, AdaGradRDA, Adam, Pegasos.")</span>
<span class="source-line-no">050</span><span id="line-50">    public double learningRate = 0.18;</span>
<span class="source-line-no">051</span><span id="line-51"></span>
<span class="source-line-no">052</span><span id="line-52">    @Option(longName = "sgo-epsilon", usage = "Epsilon for AdaDelta, AdaGrad, AdaGradRDA, Adam.")</span>
<span class="source-line-no">053</span><span id="line-53">    public double epsilon = 0.066;</span>
<span class="source-line-no">054</span><span id="line-54"></span>
<span class="source-line-no">055</span><span id="line-55">    @Option(longName = "sgo-rho", usage = "Rho for RMSProp, AdaDelta, SGD with Momentum.")</span>
<span class="source-line-no">056</span><span id="line-56">    public double rho = 0.95;</span>
<span class="source-line-no">057</span><span id="line-57"></span>
<span class="source-line-no">058</span><span id="line-58">    @Option(longName = "sgo-lambda", usage = "Lambda for Pegasos.")</span>
<span class="source-line-no">059</span><span id="line-59">    public double lambda = 1e-2;</span>
<span class="source-line-no">060</span><span id="line-60"></span>
<span class="source-line-no">061</span><span id="line-61">    @Option(longName="sgo-parameter-averaging",usage="Use parameter averaging.")</span>
<span class="source-line-no">062</span><span id="line-62">    public boolean paramAve = false;</span>
<span class="source-line-no">063</span><span id="line-63"></span>
<span class="source-line-no">064</span><span id="line-64">    @Option(longName="sgo-momentum",usage="Use momentum in SGD.")</span>
<span class="source-line-no">065</span><span id="line-65">    public SGD.Momentum momentum = SGD.Momentum.NONE;</span>
<span class="source-line-no">066</span><span id="line-66"></span>
<span class="source-line-no">067</span><span id="line-67">    /**</span>
<span class="source-line-no">068</span><span id="line-68">     * Gets the configured gradient optimiser.</span>
<span class="source-line-no">069</span><span id="line-69">     * @return The gradient optimiser.</span>
<span class="source-line-no">070</span><span id="line-70">     */</span>
<span class="source-line-no">071</span><span id="line-71">    public StochasticGradientOptimiser getOptimiser() {</span>
<span class="source-line-no">072</span><span id="line-72">        StochasticGradientOptimiser sgo;</span>
<span class="source-line-no">073</span><span id="line-73">        switch(optimiserType) {</span>
<span class="source-line-no">074</span><span id="line-74">            case ADADELTA:</span>
<span class="source-line-no">075</span><span id="line-75">                sgo = new AdaDelta(rho,epsilon);</span>
<span class="source-line-no">076</span><span id="line-76">                break;</span>
<span class="source-line-no">077</span><span id="line-77">            case ADAGRAD:</span>
<span class="source-line-no">078</span><span id="line-78">                sgo = new AdaGrad(learningRate, epsilon);</span>
<span class="source-line-no">079</span><span id="line-79">                break;</span>
<span class="source-line-no">080</span><span id="line-80">            case ADAGRADRDA:</span>
<span class="source-line-no">081</span><span id="line-81">                sgo = new AdaGradRDA(learningRate, epsilon);</span>
<span class="source-line-no">082</span><span id="line-82">                break;</span>
<span class="source-line-no">083</span><span id="line-83">            case ADAM:</span>
<span class="source-line-no">084</span><span id="line-84">                sgo = new Adam(learningRate,epsilon);</span>
<span class="source-line-no">085</span><span id="line-85">                break;</span>
<span class="source-line-no">086</span><span id="line-86">            case PEGASOS:</span>
<span class="source-line-no">087</span><span id="line-87">                sgo = new Pegasos(learningRate,lambda);</span>
<span class="source-line-no">088</span><span id="line-88">                break;</span>
<span class="source-line-no">089</span><span id="line-89">            case RMSPROP:</span>
<span class="source-line-no">090</span><span id="line-90">                sgo = new RMSProp(learningRate,rho);</span>
<span class="source-line-no">091</span><span id="line-91">                break;</span>
<span class="source-line-no">092</span><span id="line-92">            case CONSTANTSGD:</span>
<span class="source-line-no">093</span><span id="line-93">                sgo = SGD.getSimpleSGD(learningRate,rho,momentum);</span>
<span class="source-line-no">094</span><span id="line-94">                break;</span>
<span class="source-line-no">095</span><span id="line-95">            case LINEARSGD:</span>
<span class="source-line-no">096</span><span id="line-96">                sgo = SGD.getLinearDecaySGD(learningRate,rho,momentum);</span>
<span class="source-line-no">097</span><span id="line-97">                break;</span>
<span class="source-line-no">098</span><span id="line-98">            case SQRTSGD:</span>
<span class="source-line-no">099</span><span id="line-99">                sgo = SGD.getSqrtDecaySGD(learningRate,rho,momentum);</span>
<span class="source-line-no">100</span><span id="line-100">                break;</span>
<span class="source-line-no">101</span><span id="line-101">            default:</span>
<span class="source-line-no">102</span><span id="line-102">                throw new IllegalArgumentException("Unhandled StochasticGradientOptimiser type: "+optimiserType);</span>
<span class="source-line-no">103</span><span id="line-103">        }</span>
<span class="source-line-no">104</span><span id="line-104">        if (paramAve) {</span>
<span class="source-line-no">105</span><span id="line-105">            logger.info("Using parameter averaging");</span>
<span class="source-line-no">106</span><span id="line-106">            return new ParameterAveraging(sgo);</span>
<span class="source-line-no">107</span><span id="line-107">        } else {</span>
<span class="source-line-no">108</span><span id="line-108">            return sgo;</span>
<span class="source-line-no">109</span><span id="line-109">        }</span>
<span class="source-line-no">110</span><span id="line-110">    }</span>
<span class="source-line-no">111</span><span id="line-111"></span>
<span class="source-line-no">112</span><span id="line-112">}</span>




























































</pre>
</div>
</main>
</body>
</html>
